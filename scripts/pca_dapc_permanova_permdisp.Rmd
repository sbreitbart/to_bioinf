# Set up notebook
## Load libraries & functions
```{r warning=F}
source(here::here("libraries.R"))
source(here::here("functions.R"))
```

# PCA
## Import files
```{r}
# pca data
pca <- read_table(
  here::here("./results/plink_files_mmaf0.05_R0.75/mmaf0.05_R0.75.eigenvec"), 
  col_names = FALSE)

eigenval <- scan(here::here("./results/plink_files_mmaf0.05_R0.75/mmaf0.05_R0.75.eigenval"))

# urbanization data
urb <- read.csv(here("./clean_data/urb_metrics.csv")) %>%
  dplyr::rename("pop_id" = "patch_id")
```

## Find number of clusters
https://speciationgenomics.github.io/pca/
```{r}
# give our pca data.frame proper column names
pca %<>%
  dplyr::rename(
    "pop_id" = 1,
    "ind" = 2)
names(pca)[3:ncol(pca)] <- paste0("PC", 1:(ncol(pca)-2))

# create pop column that adds "MW0" or "MW00" to populations in pca data that lists some pops as simply "8" instead of "MW008", for instance
# take entries with numeric populations and add "MW" as prefix
pca <- add_MW_IDs(pca) %>%
  dplyr::select(-"pop_id") %>%
  dplyr::rename("pop_id" = "patch_id")



# first convert to percentage variance explained
pve <- data.frame(PC = 1:20,
                  pve = eigenval/sum(eigenval)*100)

# join urbanization data
pca <- dplyr::left_join(pca,
                         urb %>%
                           dplyr::select(
                             c("pop_id",
                               "urb_rur",
                               "urb_score",
                               "City_dist")),
                         by = "pop_id" )

# make plot
ggplot(pve,
       aes(PC, pve)) +
  geom_bar(stat = "identity") +
  ylab("Percentage variance explained") +
  ggpubr::theme_pubr()

# calculate the cumulative sum of the percentage variance explained
cumsum(pve$pve)


# there are many PC axes, all with rather similar amounts of variance explained. Does this mean that my data points are all rather similar? Not much variance to partition?
```

## Plot, export PCA
### Axes 1-2
```{r}
#### Urb = dist to CC
pca_dist <- ggplot(pca, 
       aes(PC1,
           PC2,
           # col = urb_score
           fill = City_dist
       )) + 
  geom_point(size = 2,
             shape = 21,
             color = "black") +
 # scale_fill_gradientn(colours=rainbow(4)) +
  scale_fill_viridis(option="plasma") +
 # scale_fill_gradientn(colours=cm.colors(5)) +
  coord_equal() +
  xlab(paste0("PC1 (", signif(pve$pve[1], 3), "%)")) +
  ylab(paste0("PC2 (", signif(pve$pve[2], 3), "%)")) +
  ggpubr::theme_pubr() +
  labs(fill = "    Distance to\nCity Center (km)")


#### Urb = urb score
pca_usc <- ggplot(pca, 
       aes(PC1,
           PC2,
           fill = urb_score
       )) + 
  geom_point(size = 2,
             shape = 21,
             color = "black") +
  scale_fill_viridis(option="plasma",
                     trans = "reverse"
                     ) +
  coord_equal() +
  xlab(paste0("PC1 (", signif(pve$pve[1], 3), "%)")) +
  ylab(paste0("PC2 (", signif(pve$pve[2], 3), "%)")) +
  ggpubr::theme_pubr() +
  labs(fill = "Urbanization\n      Score") 

pca_dist +
  pca_usc +
  patchwork::plot_layout(axes = "collect",
                         axis_titles = "collect") +
  plot_annotation(tag_levels = 'A')

ggsave("pca.png",
       path = "Figures_Tables/pca",
       height = 13,
       width = 15,
       units = "cm")
```

### All PC axes' plots
#### Urb = dist to CC
```{r}
# remove first 2 cols from pca
pca2 <- pca %>%
  dplyr::select(-(1:2))

# Create a list to store the plots
plot_list <- list()

# Loop through the PC axis pairs and create plots
for (i in seq(1, 20, by = 2)) {
  pc1 <- colnames(pca2)[i]
  pc2 <- colnames(pca2)[i + 1]
  
  # Check if both PC axes have non-zero variance
  if (sum(var(pca2[, c(pc1, pc2)])) > 0) {
    # Create the plot
    plot <- ggplot(pca2, aes_string(pc1, pc2, fill = "City_dist")) +
      geom_point(size = 2, shape = 21, color = "black") +
      scale_fill_viridis(option = "plasma") +
      coord_equal() +
      xlab(paste0(pc1, " (", signif(pve$pve[i], 3), "%)")) +
      ylab(paste0(pc2, " (", signif(pve$pve[i + 1], 3), "%)")) +
      theme_pubr() +
      labs(fill = "    Distance to\nCity Center (km)")
    
    # Add the plot to the list
    plot_list[[i]] <- plot
  }
}

# Remove any empty plots from the list
plot_list <- plot_list[!sapply(plot_list, is.null)]

# Combine all non-empty plots onto the same page
combined_plot <- ggarrange(plotlist = plot_list, 
                           nrow = 2,
                           ncol = 5,
                            common.legend = TRUE,
                           align = "v")

# Save the combined plot as a PDF file
ggsave(path = "Figures_Tables/pca",
      "pc_plots.pdf",
       combined_plot,
       width = 14,
       height = 7)

```

#### Urb = urb score
```{r}
# remove first 2 cols from pca
pca2 <- pca %>%
  dplyr::select(-(1:2))

# Create a list to store the plots
plot_list_usc <- list()

# Loop through the PC axis pairs and create plots
for (i in seq(1, 20, by = 2)) {
  pc1 <- colnames(pca2)[i]
  pc2 <- colnames(pca2)[i + 1]
  
  # Check if both PC axes have non-zero variance
  if (sum(var(pca2[, c(pc1, pc2)])) > 0) {
    # Create the plot
    plot <- ggplot(pca2, aes_string(pc1, pc2, fill = "urb_score")) +
      geom_point(size = 2, shape = 21, color = "black") +
      scale_fill_viridis(option="plasma",
                     trans = "reverse"
                     ) +
      coord_equal() +
      xlab(paste0(pc1, " (", signif(pve$pve[i], 3), "%)")) +
      ylab(paste0(pc2, " (", signif(pve$pve[i + 1], 3), "%)")) +
      theme_pubr() +
      labs(fill = "Urbanization\n      Score") 
    
    # Add the plot to the list
    plot_list_usc[[i]] <- plot
  }
}

# Remove any empty plots from the list
plot_list_usc <- plot_list_usc[!sapply(plot_list_usc, is.null)]

# Combine all non-empty plots onto the same page
combined_plot_usc <- ggarrange(plotlist = plot_list_usc, 
                            nrow = 2,
                           ncol = 5,
                            common.legend = TRUE,
                           align = "v")

# Save the combined plot as a PDF file
ggsave(path = "Figures_Tables/pca",
      "pc_plots_usc.pdf",
       combined_plot_usc,
       width = 14,
       height = 7)

```

# DAPC
## Import files
```{r}
# pop map
pops <- read.csv(here("./genomic_resources/pop_map3.csv"))

n_unique_pops <- pops %>%
  dplyr::select(population) %>%
  dplyr::summarise(n_pops = unique(population)) %>%
  dplyr::summarise(n_pops = n()) %>%
  as.numeric() %T>% print

# vcf
vcf <- read.vcfR(
  here::here("./clean_data/stacks_output/output_populations_no-IBD/mmaf0.05_R0.75/populations.snps.vcf"),
  verbose = FALSE)

## Convert vcf to genind object
my_genind <- vcfR2genind(vcf)


# add pops to genind
pop(my_genind) <- pops$population
```

## Find number of clusters
### find.clusters()
```{r}
# This function displays a graph of cumulated variance explained by the eigenvalues of the PCA.

grp <- adegenet::find.clusters(my_genind,
    # since I've been through this and seen that the lowest BIC is for ~2 clusters, I'm making this smaller to see the detail better
                        max.n.clust = 10,
                      #  max.n.clust = 26,
                        var.contrib = TRUE, 
                        scale = FALSE,
                        n.da = nPop(my_genind) - 1
                        )

# The adegenet tutorial says: "Apart from computational time, there is no reason for keeping a small number of components." 
# I'll choose n = 256- all the PCs.

# Next, it asks for the number of clusters - here select the point with the lowest BIC.
# The curve is nearly linear, increasing from 2 up to 120. I'll choose 2.
# NOTE: upon returning, I can choose 1. It has the lowest BIC, but I can't plot it, so going through this analysis choosing 2 allows for a comparison.
```

In the manual, they provide a U-shaped curve that plateaus for ~30 estimates of x. For cases like this, the authors state:

"Although the most frequently asked when trying to find clusters in genetic data, this question is equally often meaningless. Clustering algorithms help making a caricature of a complex reality, which is most of the time far from following known population genetics models. Therefore, we are rarely looking for actual panmictic populations from which the individuals have been drawn. Genetic clusters can be biologically meaningful structures and reflect interesting biological processes, but they are still models.

A slightly different but probably more meaningful question would be: ”How many clusters are useful to describe the data?”. A fundamental point in this question is that clusters are merely tools used to summarise and understand the data. There is no longer a ”true k”, but some values of k are better, more efficient summaries of the data than others. For instance, in the following case:, the concept of ”true k” is fairly hypothetical. This does not mean that clutering algorithms should necessarily be discarded, but surely the reality is more complex than a few clear-cut, isolated populations. What the BIC decrease says is that 10-20 clusters would provide useful summaries of the data. The actual number retained is merely a question of personnal taste."

### Clusters = 2
#### Examine clusters
```{r}
head(grp$Kstat)  # BIC scores per k
#     K=1      K=2      K=3      K=4      K=5      K=6 
#   1252.613 1255.311 1258.385 1261.854 1265.193 1268.666 

#  this does show that k = 1 has the lowest BIC. But I can't plot that- only at least 2 clusters on a DAPC plot. But keep that in mind.

grp$stat  # Best k based on BIC

# Group membership (based on your defined number of clusters)
group_membership_k2 <- grp$grp %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  dplyr::rename("sample" = 1,
                "cluster" = 2) %>%
  dplyr::full_join(.,
                   pops) %>%
  dplyr::rename("pop_id" = "population") %>%
  add_MW_IDs() %>%
  dplyr::select(-pop_id) %>%
  dplyr::rename("pop_id" = "patch_id") %>%
  dplyr::left_join(.,
                   urb %>%
                     dplyr::select(3,4,5,6,9,10) ,
                   by = "pop_id")

grp$size  # Group size per k cluster


find_hull <- function(df) df[chull(df$long, df$lat), ]
hulls_k2 <- plyr::ddply(group_membership_k2, "cluster", find_hull)

# plot clusters on map
get_stamenmap(c(left = -80.5, bottom = 43.2, right = -78.5, top = 44.15),
              zoom = 10,
              maptype = "terrain") %>%
  ggmap() +
  geom_point(data = group_membership_k2,
             aes(x = long,
             y = lat,
             fill = cluster),
             size = 2,
             color = "black",
             shape = 21) +
    geom_polygon(data = hulls_k2,
                 mapping = aes_string(
                   x = "long", 
                   y = "lat", 
                   color = as.factor(hulls_k2$cluster), 
                   fill = as.factor(hulls_k2$cluster)),
                 alpha = 0.25) +
  xlab("Longitude") +
  ylab("Latitude") +
  scale_fill_manual(values = c("blue", "red")) +
  scale_color_manual(values = c( "blue", "red")) +
  labs(fill = "Cluster",
       color = "Cluster")

ggsave(here::here("./Figures_Tables/dapc/k2_map.png"),
         width = 16,
         height = 11,
         units = "cm")

# violin plot of clusters vs urbanization
ggplot(group_membership_k2,
       aes(y = City_dist,
       x = cluster)) +
  geom_violin(aes(y = City_dist,
             x = cluster,
             color = cluster,
             fill = cluster),
             alpha = 0.4,
             draw_quantiles = c(0.25, 0.5, 0.75)) +
  geom_dotplot(aes(y = City_dist,
             x = cluster,
             fill = cluster),
             binaxis = "y",
               stackdir = "center",
               dotsize = 0.5,
             binwidth = 4) +
  ylab("Distance to City Center") +
  scale_fill_manual(values = c("1" = "blue",
                      "2" = "red")) +
  scale_color_manual(values = c("1" = "blue",
                      "2" = "red")) +
  ggpubr::theme_pubr()

ggsave(here::here("./Figures_Tables/dapc/k2_dotplot.png"),
         width = 16,
         height = 11,
         units = "cm")
```

#### Plot clusters
```{r}
dapc1 <- adegenet::dapc(my_genind, grp$grp)
# The method displays the same graph of cumulated variance as in find.cluster. However, unlike k-means, DAPC can benefit from not using too many PCs. Indeed, retaining too many components with respect to the number of individuals can lead to over-fitting and unstability in the membership probabilities returned by the method.
# 
# I'll choose 100.


# Then, the method displays a barplot of eigenvalues for the discriminant analysis, asking for a number of discriminant functions to retain (unless argument n.da is provided).
# 
# Looks like there's only one eigenvalue. In this case:
# 
# "Scatter can also represent a single discriminant function, which is especially useful when only one of these has been retained (e.g. in the case k = 2). This is achieved by plotting the densities of individuals on a given discriminant function with different colors for different groups."

png(filename = "Figures_Tables/dapc/dapc_clusters_2.png",
    height = 400,
    width = 600,
    units = "px")

scatter(dapc1,
        scree.da=FALSE,
        bg="white",
        solid=0.4,
        legend = TRUE
      ) 
  

dev.off()



# plot discriminant functions in space
dapc1_df <- dapc1$ind.coord %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  rename("sample" = 1) %>%
  dplyr::full_join(.,
                   pops,
                   by = "sample") %>%
  dplyr::rename("pop_id" = "population") %>%
  add_MW_IDs() %>%
  dplyr::select(-pop_id) %>%
  dplyr::rename("pop_id" = "patch_id") %>%
  dplyr::left_join(.,
                   urb %>%
                     dplyr::select(3,4,5,6,9,10) ,
                   by = "pop_id")


get_stamenmap(c(left = -80.5, bottom = 43.2, right = -78.5, top = 44.15),
              zoom = 10,
              maptype = "terrain") %>%
  ggmap(
    # make basemap lighter
    darken = c(0.5, "white")
    ) +
  geom_point(data = dapc1_df,
             aes(x = long,
             y = lat,
             fill = LD1),
             size = 2.5,
             color = "black",
              shape = 21) +
    # geom_polygon(data = hulls_k2,
    #              mapping = aes_string(
    #                x = "long", 
    #                y = "lat", 
    #                color = as.factor(hulls_k2$cluster), 
    #                fill = as.factor(hulls_k2$cluster)),
    #              alpha = 0.3) +
  xlab("Longitude") +
  ylab("Latitude") +
  scale_fill_gradient2(
    high = "red",
    mid = "white",
    low = "blue",
    midpoint = 0,
    limits = c(-5, 5),
  #  name ='PC coordinates',
    name ='Discriminant \nfunction 1',
    #breaks=c(5, 2.5, 0, -2.5, -5),
    labels=c(-5, "-2.5 (Cluster 1)", 0, "2.5 (Cluster 2)", 5))
                      

ggsave(here::here("./Figures_Tables/dapc/k2_dapc_map.png"),
         width = 16,
         height = 11,
         units = "cm")
```


### Clusters = 1
#### Examine clusters
```{r}
head(grp$Kstat)  # BIC scores per k
#      K=1      K=2      K=3      K=4      K=5      K=6 
# 1253.280 1256.087 1259.100 1262.365 1266.032 1269.594 


grp$stat  # Best k based on BIC
#     K=1 
# 1253.28 
```


#### Create fig of variance explained and BIC
```{r}
var_exp <- rasterGrob(readPNG(here::here("./Figures_Tables/dapc/variance_expl_500by350.png")))

bic <- rasterGrob(readPNG(here::here("./Figures_Tables/dapc/BIC_500by350.png")))


dapc_plots <- gridExtra::grid.arrange(var_exp, 
                        bic, 
                        ncol = 2,
             left = textGrob("A", 
                            x = unit(0.95, "npc"), 
                            y = unit(.9, "npc"),
                          #  just = "left", 
                            gp = gpar(fontsize = 12)),
             right = textGrob("B", 
                            x = unit(-15.5, "npc"), 
                            y = unit(0.9, "npc"),
                          #  just = "left", 
                            gp = gpar(fontsize = 12)))

ggsave("./Figures_Tables/dapc/dapc_both.png", 
       plot = dapc_plots,
       width = 7, height = 2.5,
       units = "in",
       dpi = 300)
```

## Cross-validation

Carrying out a DAPC requires the user to define the number of PCs retained in the analysis. As discussed above, this is not a trivial decision, as the number of PCs can have a substantial
impact on the results of the analysis. Cross-validation (carried out with the function xvalDapc) provides an objective optimisation procedure for identifying the ’golidlocks point’
in the trade-off between retaining too few and too many PCs in the model. In cross-validation, the data is divided into two sets: a training set (typically comprising 90% of the data) and a validation set (which contains the remainder (by default, 10%) of the data). With xvalDapc, the validation set is selected by stratified random sampling: this ensures that at least one member of each group or population in the original data is represented in both training and validation sets.

DAPC is carried out on the training set with variable numbers of PCs retained, and the degree to which the analysis is able to accurately predict the group membership of excluded individuals (those in the validation set) is used to identify the optimal number of PCs to retain. At each level of PC retention, the sampling and DAPC procedures are repeated n.rep times. (By default, we perform 30 replicates, though it should be noted that for large datasets, performing large numbers of replicates may be computationally intensive).

```{r}
mat <- tab(my_genind, NA.method="mean")
grp <- pop(my_genind)
xval <- xvalDapc(mat,
                 grp,
                 training.set = 0.9,
                 result = "groupMean",
                 center = TRUE,
                 scale = FALSE,
                 n.pca = NULL,
                 n.rep = 100, 
                 xval.plot = TRUE)

# Warning message:
# In xvalDapc.matrix(mat, grp, training.set = 0.9, result = "groupMean",  :
#   77 groups have only 1 member: these groups cannot be represented in both training and validation sets.
```

When xval.plot is TRUE, a scatterplot of the DAPC cross-validation is generated. The number of PCs retained in each DAPC varies along the x-axis, and the proportion of successful outcome prediction varies along the y-axis. Individual replicates appear as points, and the
density of those points in different regions of the plot is displayed in blue.
As one might expect (or hope) for an optimisation procedure, the results of cross-validation here take on an arc-like shape. Predictive success is sub-optimal with both too few and too many retained PCA axes. At the apex of this arc, we that we are able to achieve 60% - 70% predictive success and an associated root mean squared error (RMSE) of 30% - 40%.
While in this example, the number of PCs associated with the highest mean success is also associated with the lowest MSE, this is not always the case. Based on the model validation literature, we recommend using the number of PCs associated with the lowest RMSE as the ’optimum’ n.pca in the DAPC analysis. Hence, we return this dapc object as the seventh component of the output of xvalDapc.

```{r}
xval[2:6]

# looks like ~20 PCs is the optimal number.
# I'll run this xvalidation function again with only ~10-30 PCs
```

```{r}
xval2 <- xvalDapc(mat,
                 grp,
                 training.set = 0.9,
                 result = "groupMean",
                 center = TRUE,
                 scale = FALSE,
                 n.pca = 10:30,
                 n.rep = 100, 
                 xval.plot = TRUE)

xval2[2:6]

# 19 PCs is the optimal number.

scatter(xval[7]$DAPC,
        clabel = FALSE)

# this looks like a mess. I'm concerned that so many populations couldn't be included in the cross-validation because there was only 1 individual for 77 populations. So I don't trust this very much.
# And again, as stated earlier in the tutorial, "Apart from computational time, there is no reason for keeping a small number of components". So I'll keep doing what I was doing earlier and using all the PCs.
```

# PERMANOVA (distance-based MANOVA)
First, generate "pca" object under "PCA" heading
## Analysis
### Urbanization is quantitative
#### Distance to CC
```{r}
# this does the same thing as if I created my own distance matrix and replaced the pca[,3:22] with it:
# dist_matrix <- dist(pca[, 3:22], method = "euclidean")
adonis_dist1 <- vegan::adonis2(formula = pca[,3:22] ~ City_dist,
               data = pca,
               permutations = 1000,
               method = "euclidean",
               # control for sampling indivs within the same pop
               strata = pca$pop_id)

# not significant
```

#### Urb Score
```{r}

adonis_usc1 <- vegan::adonis2(formula = pca[,3:22] ~ urb_score,
               data = pca,
               permutations = 1000,
               method = "euclidean",
               # control for sampling indivs within the same pop
               strata = pca$pop_id)

# same result

```

### Urbanization is categorical
#### Create columns categorizing sites as urban or rural
```{r}
pca %<>%
  dplyr::mutate(u_r_dist = case_when(
    City_dist <= 30 ~ "Urban",
    TRUE ~ "Rural")) %>%
  dplyr::mutate(u_r_usc = case_when(
    urb_score > 0 ~ "Urban",
    TRUE ~ "Rural"))
```

#### Distance to CC
```{r}

adonis_dist2 <- vegan::adonis2(formula = pca[,3:22] ~ u_r_dist,
               data = pca,
               permutations = 1000,
               method = "euclidean",
               # control for sampling indivs within the same pop
               strata = pca$pop_id)

# qualitatively identical results as above

```

#### Urb Score
```{r}

adonis_usc2 <- vegan::adonis2(formula = pca[,3:22] ~ u_r_usc,
               data = pca,
               permutations = 1000,
               method = "euclidean",
               # control for sampling indivs within the same pop
               strata = pca$pop_id)

# same result as above

```

## Export
### Create function
```{r}
adonis_export <- function(adonis_obj, output_file_name){
  adonis_obj %>%
    as.data.frame() %>%
    rownames_to_column() %>%
    dplyr::rename("Variable" = 1,
                  "df" = 2,
                  "SS" = 3,
                  "p" = 6) %>%
    dplyr::mutate_if(is.numeric, round, 3) %>%
    dplyr::mutate(Variable = case_when(
      Variable == 'City_dist' ~ 'Distance to City Center',
      Variable == 'urb_score' ~ 'Urbanization Score',     
      Variable == 'u_r_dist' ~ 'Distance to City Center',
      Variable == 'u_r_usc' ~ 'Urbanization Score',
      TRUE ~ Variable)) %>%
    flextable() %>%
    flextable::compose(i = 1, j = 4, part = "header",
                       value = as_paragraph("R", as_sup("2"))) %>%  autofit() %>%
    save_as_docx(path = here::here(paste0("./Figures_Tables/permanova_permdisp/",
                                          output_file_name,
                                          ".docx")))
}
```

### Export
```{r}
adonis_export(adonis_dist1, "permanova_dist_quant")
adonis_export(adonis_usc1, "permanova_usc_quant")
adonis_export(adonis_dist2, "permanova_dist_categ")
adonis_export(adonis_usc2, "permanova_usc_categ")
```

# PERMDISP (beta dispersal)
multivariate extension of Levene’s test for homogeneity of variances
## Urb = dist
```{r}
# create distance object, officially
dist_matrix <- dist(pca[, 3:22], method = "euclidean")


# type = median
bd <- vegan::betadisper(d = dist_matrix, 
                  group = pca$u_r_dist,
                  type = "median"
                  )

# this shows that there is weak evidence that there is more variation within one group vs the other
anova(bd)

# looking at bd, we can see the urban group has a slightly higher average distance to the median than the rural
# # Average distance to median:
#  Rural  Urban 
# 0.2618 0.2763 

# % difference? 5.5%
((0.2763-0.2618)/0.2618)*100


# type = centroid- what I'm citing in the paper
bd_centroid <- vegan::betadisper(d = dist_matrix, 
                  group = pca$u_r_dist,
                  type = "centroid"
                  )

# this shows that there is weak evidence that there is more variation within one group vs the other (nearly identical p-value to result above- 0.07)
anova(bd_centroid)

# get R2
tidy_aov <- tidy(anova(bd_centroid))
sum_squares_regression <- tidy_aov$sumsq[1]
sum_squares_residuals <- tidy_aov$sumsq[2]

R_squared <- sum_squares_regression /
            (sum_squares_regression + sum_squares_residuals)

R_squared # 0.01263263


# looking at bd, we can see the urban group has a slightly higher average distance to the centroid than the rural
# Average distance to centroid:
#  Rural  Urban 
# 0.2620 0.2765 

# % difference? 5.5%
((0.2765-0.2620)/0.2620)*100


plot(bd, ellipse = TRUE, hull = FALSE)
plot(bd_centroid, ellipse = TRUE, hull = FALSE)

# urban individuals: 150
pca %>% dplyr::filter(u_r_dist == "Urban") %>% dim()

# rural individuals: 106
pca %>% dplyr::filter(u_r_dist == "Rural") %>% dim()
```

## Urb = urb score
```{r}
# type = median
bd_usc <- vegan::betadisper(d = dist_matrix, 
                  group = pca$u_r_usc,
                  type = "median"
                  )

# this shows that there is weak evidence that there is NOT more variation within one group vs the other
anova(bd_usc)

# looking at bd_usc, we can see the urban group has a slightly higher average distance to the median than the rural
# # Average distance to median:
#  Rural  Urban 
# 0.2643 0.2752 


# type = centroid- what I'm citing in the paper
bd_centroid_usc <- vegan::betadisper(d = dist_matrix, 
                  group = pca$u_r_usc,
                  type = "centroid"
                  )

# this shows that there is NOT evidence that there is more variation within one group vs the other
anova(bd_centroid_usc)

# get R2
tidy_aov_usc <- tidy(anova(bd_centroid_usc))
sum_squares_regression_usc <- tidy_aov_usc$sumsq[1]
sum_squares_residuals_usc <- tidy_aov_usc$sumsq[2]

R_squared_usc <- sum_squares_regression_usc /
            (sum_squares_regression_usc + sum_squares_residuals_usc)

R_squared_usc # 0.007082197



# looking at bd_centroid_usc, we can see the urban group has a slightly higher average distance to the centroid than the rural
# Average distance to centroid:
#  Rural  Urban 
# 0.2645 0.2753 

plot(bd_usc, ellipse = TRUE, hull = FALSE)
plot(bd_centroid_usc, ellipse = TRUE, hull = FALSE)

# urban individuals: 141
pca %>% dplyr::filter(u_r_usc == "Urban") %>% dim()

# rural individuals: 115
pca %>% dplyr::filter(u_r_usc == "Rural") %>% dim()
```


## Export
### Create function to export anovas
```{r}
bd_export <- function(betadisper_obj, output_file_name){

  betadisper_obj %>%
  anova() %>%
    as.data.frame() %>%
    rownames_to_column() %>%
    dplyr::rename("Variable" = 1,
                  "df" = 2,
                  "SS" = 3,
                  "MS" = 4,
                  "F" = 5,
                  "p" = 6) %>%
    dplyr::mutate_if(is.numeric, round, 3) %>%
    dplyr::mutate(Variable = case_when(
      Variable == 'Groups' ~ 'Urban/Rural Groups',
      TRUE ~ Variable)) %>%
    flextable() %>%
    flextable::align(j = c(2:6), part = "all", align = "center") %>%
  autofit() %>%
    save_as_docx(path = here::here(paste0("./Figures_Tables/permanova_permdisp/",
                                          output_file_name,
                                          ".docx")))
}

```

### Export anovas
```{r}
bd_export(bd_centroid, "permdisp_anova_dist")
bd_export(bd_centroid_usc, "permdisp_anova_usc")
```

### Export plots
```{r}
# Dist to CC
png(here::here("./Figures_Tables/permanova_permdisp/permdisp_centroid_dist.png"),
    height = 400,
    width = 400)
plot(bd_centroid, ellipse = TRUE, hull = FALSE)
dev.off()

# Urb score
png(here::here("./Figures_Tables/permanova_permdisp/permdisp_centroid_urbscore.png"),
    height = 400,
    width = 400)
plot(bd_centroid_usc, ellipse = TRUE, hull = FALSE)
dev.off()
```
