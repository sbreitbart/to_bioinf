# Set up notebook
## Load libraries & functions
```{r warning=F}
source(here::here("libraries.R"))
source(here::here("functions.R"))
```

# PCA
## Import files
```{r}
# pca data
pca <- read_table(
  here::here("./plink_files_mmaf0.05_R1/mmaf0.05_R1.eigenvec"), 
  col_names = FALSE)

eigenval <- scan(here::here("./plink_files_mmaf0.05_R1/mmaf0.05_R1.eigenval"))


# urbanization data
urb <- read.csv(here("./clean_data/urb_metrics.csv")) %>%
  dplyr::rename("pop_id" = "patch_id")
```

## Find number of clusters
https://speciationgenomics.github.io/pca/
```{r}
# give our pca data.frame proper column names
pca %<>%
  dplyr::rename(
    "pop_id" = 1,
    "ind" = 2)
names(pca)[3:ncol(pca)] <- paste0("PC", 1:(ncol(pca)-2))

# create pop column that adds "MW0" or "MW00" to populations in pca data that lists some pops as simply "8" instead of "MW008", for instance
# take entries with numeric populations and add "MW" as prefix
pca <- add_MW_IDs(pca) %>%
  dplyr::select(-"pop_id") %>%
  dplyr::rename("pop_id" = "patch_id")



# first convert to percentage variance explained
pve <- data.frame(PC = 1:20,
                  pve = eigenval/sum(eigenval)*100)

# join urbanization data
pca <- dplyr::left_join(pca,
                         urb %>%
                           dplyr::select(
                             c("pop_id",
                               "urb_rur",
                               "urb_score",
                               "City_dist")),
                         by = "pop_id" )

# make plot
ggplot(pve,
       aes(PC, pve)) +
  geom_bar(stat = "identity") +
  ylab("Percentage variance explained") +
  ggpubr::theme_pubr()

# calculate the cumulative sum of the percentage variance explained
cumsum(pve$pve)

# plot pca
ggplot(pca, 
       aes(PC1,
           PC2,
           # col = urb_score
           fill = City_dist
       )) + 
  geom_point(size = 2,
             shape = 21,
             color = "black") +
 # scale_fill_gradientn(colours=rainbow(4)) +
  scale_fill_gradientn(colours=heat.colors(5)) +
 # scale_fill_gradientn(colours=cm.colors(5)) +
  coord_equal() +
  xlab(paste0("PC1 (", signif(pve$pve[1], 3), "%)")) +
  ylab(paste0("PC2 (", signif(pve$pve[2], 3), "%)")) +
  ggpubr::theme_pubr()

ggsave("pca.png",
       path = "Figures_Tables/pca")
```

# DAPC
## Import files
```{r}
# pop map
pops <- read.csv(here("./genomic_resources/pop_map2.csv"))
n_unique_pops <- pops %>%
  dplyr::select(population) %>%
  dplyr::summarise(n_pops = unique(population)) %>%
  dplyr::summarise(n_pops = n()) %>%
  as.numeric() %T>% print

# vcf
vcf <- read.vcfR(
  here::here("./rounds3_4_all_vcfs/mmaf0.05_R1/mmaf0.05_R1.vcf"),
  verbose = FALSE)

## Convert vcf to genind object
my_genind <- vcfR2genind(vcf)


# add pops to genind
pop(my_genind) <- pops$population
```

## Find number of clusters
### find.clusters()
```{r}
# This function displays a graph of cumulated variance explained by the eigenvalues of the PCA.

grp <- adegenet::find.clusters(my_genind,
                        # max clusters = number of sample site populations = 123
                        max.n.clust= n_unique_pops, 
                        var.contrib = TRUE, 
                        scale = FALSE,
                        n.da = nPop(my_genind) - 1
                        )

# The adegenet tutorial says: "Apart from computational time, there is no reason for keeping a small number of components." 
# However, I get error messages about non-convergence if I supply a number higher than the number of components.
# I see the curve tapering off around 100 PC, so I'll supply that number.

# Next, it asks for the number of clusters - here select the point with the lowest BIC.
# 2 and 3 look extremely close based on lowest BIC in graph. I'll choose 2.
```

In the manual, they provide a U-shaped curve that plateaus for ~30 estimates of x. For cases like this, the authors state:

"Although the most frequently asked when trying to find clusters in genetic data, this question is equally often meaningless. Clustering algorithms help making a caricature of a complex reality, which is most of the time far from following known population genetics models. Therefore, we are rarely looking for actual panmictic populations from which the individuals have been drawn. Genetic clusters can be biologically meaningful structures and reflect interesting biological processes, but they are still models.

A slightly different but probably more meaningful question would be: ”How many clusters are useful to describe the data?”. A fundamental point in this question is that clusters are merely tools used to summarise and understand the data. There is no longer a ”true k”, but some values of k are better, more efficient summaries of the data than others. For instance, in the following case:, the concept of ”true k” is fairly hypothetical. This does not mean that clutering algorithms should necessarily be discarded, but surely the reality is more complex than a few clear-cut, isolated populations. What the BIC decrease says is that 10-20 clusters would provide useful summaries of the data. The actual number retained is merely a question of personnal taste."

### Clusters = 2
#### Examine clusters
```{r}
head(grp$Kstat)  # BIC scores per k
# k = (1,2,3,4) all have BIC within 2 BIC from the lowest (k = 2)
#    K=1      K=2      K=3      K=4     
# 848.0387 846.5077 846.9920 848.2155

grp$stat  # Best k based on BIC
grp$grp  # Group membership (based on your defined number of clusters)
grp$size  # Group size per k cluster
```

#### Plot clusters
```{r}
dapc1 <- adegenet::dapc(my_genind, grp$grp)
# The method displays the same graph of cumulated variance as in find.cluster. However, unlike k-means, DAPC can benefit from not using too many PCs. Indeed, retaining too many components with respect to the number of individuals can lead to over-fitting and unstability in the membership probabilities returned by the method.
# 
# I'll choose 60.


# Then, the method displays a barplot of eigenvalues for the discriminant analysis, asking for a number of discriminant functions to retain (unless argument n.da is provided).
# 
# Looks like there's only one eigenvalue. In this case:
# 
# "Scatter can also represent a single discriminant function, which is especially useful when only one of these has been retained (e.g. in the case k = 2). This is achieved by plotting the densities of individuals on a given discriminant function with different colors for different groups."

png(filename = "Figures_Tables/dapc/dapc_clusters_2.png")

scatter(dapc1,
        scree.da=FALSE,
        bg="white",
        solid=0.4,
        legend = TRUE
      ) 

dev.off()
```


### Clusters = 1- didn't work
#### find.clusters()
```{r}

grp2 <- adegenet::find.clusters(my_genind,
                        max.n.clust= 1, 
                        var.contrib = TRUE, 
                        scale = FALSE,
                        n.da = nPop(my_genind) - 1
                        )

# PC = 100

# num clusters = 1
```

#### Examine clusters
```{r}
head(grp2$Kstat)  # BIC scores per k
grp2$stat  # Best k based on BIC
grp2$grp  # Group membership (based on your defined number of clusters)
grp2$size  # Group size per k cluster
```

#### Plot clusters
```{r}
dapc2 <- adegenet::dapc(my_genind, grp2$grp)

# Choose the number PCs to retain (>=1): 100
# I keep getting this error, regardless of n: "Error in svd(X, nu = 0L) : infinite or missing values in 'x'"

# So I don't think I can move forward with n = 1.
# 
# The method displays the same graph of cumulated variance as in find.cluster. However, unlike k-means, DAPC can benefit from not using too many PCs. Indeed, retaining too many components with respect to the number of individuals can lead to over-fitting and unstability in the membership probabilities returned by the method.

```

### Clusters = 3
#### find.clusters()
```{r}
grp3 <- adegenet::find.clusters(my_genind,
                        # max clusters = number of sample site populations = 123
                        max.n.clust= n_unique_pops, 
                        var.contrib = TRUE, 
                        scale = FALSE,
                        n.da = nPop(my_genind) - 1
                        )

# PC = 100

# n(clusters) = 3
```

#### Examine clusters
```{r}
head(grp3$Kstat)  # BIC scores per k
grp3$stat  # Best k based on BIC
grp3$grp  # Group membership (based on your defined number of clusters)
grp3$size  # Group size per k cluster
```

#### Plot clusters
```{r}
dapc3 <- adegenet::dapc(my_genind, grp3$grp)
# The method displays the same graph of cumulated variance as in find.cluster. However, unlike k-means, DAPC can benefit from not using too many PCs. Indeed, retaining too many components with respect to the number of individuals can lead to over-fitting and unstability in the membership probabilities returned by the method.
# 
# I'll choose 60.


# Then, the method displays a barplot of eigenvalues for the discriminant analysis, asking for a number of discriminant functions to retain (unless argument n.da is provided).
# I'll choose 2/2.

png(filename = "Figures_Tables/dapc/dapc_clusters_3.png")

scatter(dapc3,
        scree.da=FALSE,
        bg="white",
        solid=0.4
      )

dev.off()
```

### Clusters = 4
#### find.clusters()
```{r}
grp4 <- adegenet::find.clusters(my_genind,
                        # max clusters = number of sample site populations = 123
                        max.n.clust= n_unique_pops, 
                        var.contrib = TRUE, 
                        scale = FALSE,
                        n.da = nPop(my_genind) - 1
                        )

# PC = 100

# n(clusters) = 4
```

#### Examine clusters
```{r}
head(grp4$Kstat)  # BIC scores per k
grp4$stat  # Best k based on BIC
grp4$grp  # Group membership (based on your defined number of clusters)
grp4$size  # Group size per k cluster
```

#### Plot clusters
```{r}
dapc4 <- adegenet::dapc(my_genind, grp4$grp)
# The method displays the same graph of cumulated variance as in find.cluster. However, unlike k-means, DAPC can benefit from not using too many PCs. Indeed, retaining too many components with respect to the number of individuals can lead to over-fitting and unstability in the membership probabilities returned by the method.
# 
# I'll choose 60.


# Then, the method displays a barplot of eigenvalues for the discriminant analysis, asking for a number of discriminant functions to retain (unless argument n.da is provided).
# I'll choose 3/3.

png(filename = "Figures_Tables/dapc/dapc_clusters_4.png")

scatter(dapc4,
        scree.da=FALSE,
        bg="white",
        solid=0.4
      )

dev.off()
```

## Cross-validation

Carrying out a DAPC requires the user to define the number of PCs retained in the analysis. As discussed above, this is not a trivial decision, as the number of PCs can have a substantial
impact on the results of the analysis. Cross-validation (carried out with the function xvalDapc) provides an objective optimisation procedure for identifying the ’golidlocks point’
in the trade-off between retaining too few and too many PCs in the model. In cross-validation, the data is divided into two sets: a training set (typically comprising 90% of the data) and a validation set (which contains the remainder (by default, 10%) of the data). With xvalDapc, the validation set is selected by stratified random sampling: this ensures that at least one member of each group or population in the original data is represented in both training and validation sets.

DAPC is carried out on the training set with variable numbers of PCs retained, and the degree to which the analysis is able to accurately predict the group membership of excluded individuals (those in the validation set) is used to identify the optimal number of PCs to retain. At each level of PC retention, the sampling and DAPC procedures are repeated n.rep times. (By default, we perform 30 replicates, though it should be noted that for large datasets, performing large numbers of replicates may be computationally intensive).

```{r}
mat <- tab(my_genind, NA.method="mean")
grp <- pop(my_genind)
xval <- xvalDapc(mat,
                 grp,
                 training.set = 0.9,
                 result = "groupMean",
                 center = TRUE,
                 scale = FALSE,
                 n.pca = NULL,
                 n.rep = 100, 
                 xval.plot = TRUE)

# Warning message:
# In xvalDapc.matrix(mat, grp, training.set = 0.9, result = "groupMean",  :
#   78 groups have only 1 member: these groups cannot be represented in both training and validation sets.
```

When xval.plot is TRUE, a scatterplot of the DAPC cross-validation is generated. The number of PCs retained in each DAPC varies along the x-axis, and the proportion of successful outcome prediction varies along the y-axis. Individual replicates appear as points, and the
density of those points in different regions of the plot is displayed in blue.
As one might expect (or hope) for an optimisation procedure, the results of cross-validation here take on an arc-like shape. Predictive success is sub-optimal with both too few and too many retained PCA axes. At the apex of this arc, we that we are able to achieve 60% -
70% predictive success and an associated root mean squared error (RMSE) of 30% - 40%.
While in this example, the number of PCs associated with the highest mean success is also associated with the lowest MSE, this is not always the case. Based on the model validation literature, we recommend using the number of PCs associated with the lowest RMSE as the ’optimum’ n.pca in the DAPC analysis. Hence, we return this dapc object as the seventh component of the output of xvalDapc.

```{r}
xval[2:6]

# looks like ~20 PCs is the optimal number.
# I'll run this xvalidation function again with only ~10-30 PCs
```

```{r}
xval2 <- xvalDapc(mat,
                 grp,
                 training.set = 0.9,
                 result = "groupMean",
                 center = TRUE,
                 scale = FALSE,
                 n.pca = 10:30,
                 n.rep = 100, 
                 xval.plot = TRUE)

xval2[2:6]

# still, 20 PCs is the optimal number.

scatter(xval[7]$DAPC,
        clabel = FALSE)

# this looks like a mess. I'm concerned that so many populations couldn't be included in the cross-validation because there was only 1 individual for 78 populations. So I don't trust this very much.
# And again, as stated earlier in the tutorial, "Apart from computational time, there is no reason for keeping a small number of components". So I'll keep doing what I was doing earlier and using all the PCs.
```

